---
output:
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
---


```{r eval=FALSE, include=FALSE}
library(tidyverse)
options(scipen = 999) # this stops numbers appearing as scientific notation

load("survey_raw.RData")

# set up the data for Task 3
d <- 
  data %>% 
  select(uk_salary, sibling_order, home_location, home_location_in_UK, attention_check = attention_check_2) %>%
  mutate(sibling_order = case_when(str_detect(sibling_order, "oldest") ~ "oldest",
                                   str_detect(sibling_order, "middle") ~ "middle",
                                   str_detect(sibling_order, "youngest") ~ "youngest",
                                   str_detect(sibling_order, "only") ~ "only"),
         home_location_in_UK = case_when(str_detect(home_location_in_UK, "North West") ~ "NW",
                                         str_detect(home_location_in_UK, "North East") ~ "NE",
                                         str_detect(home_location_in_UK, "South West") ~ "SW",
                                         str_detect(home_location_in_UK, "South East") ~ "SE",
                                         str_detect(home_location_in_UK, "Midlands") ~ "Midlands",
                                         str_detect(home_location_in_UK, "Wales") ~ "Wales"),
         attention_check = case_when(str_detect(attention_check, "strongly dis") ~ "strongly disagree",
                                     TRUE ~ attention_check)) %>% 
  write_csv("files/Week_7/salary_data_wk7.csv")

```

# Week 7: Filtering data and testing means (one-sample t-test)

> Written by Tom Beesley & John Towse

Today we will look in a bit more detail at people's estimates of the average UK salary. We will first plot this data using `geom_histogram()` and also `geom_boxplot()`. When we do this, we'll see that there are some unusual values, and we'll need to do a bit of **data wrangling** to remove them, using the `filter()` command. We'll then turn to the conceptual ideas of the lecture - how can we tell if the mean of our sample is unusual, or whether we would actually expect this mean value under the null hypothesis? Finally, we'll continue to develop our skills in **data visualisation** by exploring `geom_density()` plots.

## Pre-lab work: online tutorial

Please make every attempt to complete this before the lab! To access the  (on campus, or VPN required) - Not available yet - try later on Friday.

- Create a folder for Week 7

2. Download the [Week_7.zip](files/Week_7/Week_7.zip) and upload it into this new folder in RStudio Server. If you need them, [here are the instructions](#uploading_zip) from Week 2.

## RStudio tasks

### Plotting and filtering

1. Open the Week_7_script and run the `library`, `options` commands. The `options` command is new. It is very cryptic and you don't need to worry too much about this - it is making sure that the values in the graphs are displayed as regular numbers and not as scientific notation. Add a `read_csv()` command to get the data - you've done this every week, so this should be familiar. 

2. Complete the `geom_histogram()` code to plot the distribution of salary data. Try setting `bins=20` within your `geom_histogram()` code. Play around with the number of bins for the histogram.

3. OK - we've got some pretty funky values here! Some people think the average salary is over £400,000!!! Well, maybe they just added too many zeros (let's give them the benefit of the doubt). Quite often when we get our "raw" data, it contains weird values like this that we need to consider removing. Let's run the `arrange()` code now to see what exactly those high values are.

4. We'll need to remove these high values to get a better sense of the distribution. Let's use a `filter()` command to do this. We need to make a decision about what values to exclude. In later labs we'll look at a more systematic process of removing *outliers*, but for now, let's just remove any that are over £200,000. Edit the `filter()` command to keep only those estimates that are *below* £200,000 (<). Remember that the filter command *keeps* the data that is *TRUE* according to the expression. 

5. STOP! OK, **this next bit is very important**. If you completed that last step correctly you'll see an output in the console showing a "tibble" (a data frame) which has 194 rows and 5 columns. However, your object has not actually changed in the environment. This will still be showing as 197 observations (row). So the filter command will take out those large estimations, but we haven't actually changed the data object. To do this we need to assign (`<-`)the result of our filter command to the object. To do this, you need to put in some code at the start of this small chunk of code you've just run, so that the result of your commands will be assigned to the object (you can make this the same object name you've been using -overwrite it-, or call it a new name).

6. When you run this command again you should see the (new) object has changed to 194 rows. We can now plot the data again as a histogram. To do this, copy the earlier code for the histogram and edit it as necessary (if you're using a new object for the filtered data, you'll need to edit the code to reflect that). .

7. And as you know, we can also look at the distribution as a boxplot, a violin, or a density plot. Feel free to add in your own code for other visualisations you might like to try. 

### One-sample t-test

We now want to know if the salary estimates are different to the actual average salary in the UK (which is approx. £30,000). Our hypothesis might be that people are inaccurate - they overestimate or underestimate the average UK salary. Let's test that.

8. First, let's calculate the mean and sd of the column.

9. Now we can compute a t-statistic and check whether this mean is significantly different from the expected mean. We do this with `t.test()`. Edit the code on this line to conduct a one-sample t-test. You need to provide **the sample of data on which you want to conduct the test**, and the **expected mean under the null hypothesis**. Remember our hypothesis is that people are not accurate. Your calculation of the mean should tell you whether they numerically overestimated or underestimated. But would we expect such a result under the null hypothesis? Run the t-test and **note the p value**. How likely is it that we would see this sample of data (this mean value and the distribution of data - the SD) under the null hypothesis? The p value ranges from 0 to 1. If it is very low - typically we say p < .05 - then we conclude our result is unlikely under the null hypothesis and it is therefore a *significant result*.

10. What is the critical value of t in the t-distribution table, for this sample size? Degrees of freedom is N - 1.


![](files/Week_7/ttable.png) 

### Practising filtering

11. Filtering can also be useful for selecting certain sub-sets of our data. In the script we have given you an example of how we select a sub-set of data based on two conditions from two different columns:

`data_set_name %>% filter(home_location_in_UK == "NW" & sibling_order == "oldest")`
  
We have given you a few different columns to look at and to use in practicing your filter commands:

`sibling_order`: what position in age was the respondent within their siblings
`home_location`: UK / Asia / Europe, etc
`home_location_in_UK`: NW, NE, etc (NA is non-UK residents)
`attention_check`: respondents were asked "click strongly agree to show you are paying attention" - some people failed this!!!

Complete the following filters. We've put in **()** the number of rows you should see in the resulting object

12. Just those people who come from the North East (16 rows)

13. Those people who come from Wales and are the middle child within their siblings (2 rows)

14. Those people passed the attention check, are from the UK, and are an only child (21 rows)

15. Those people who are NOT from the North West; you'll need to use `!=` (84 rows)

16. Those people who failed the attention check (that is, did not say strongly agree) (14 rows)

17 Those people who are from the South East or (`|`) the South West (38 rows)

## Sample size, size of effect, and the one sample t-test

In the lecture this week....

[**Shiny demo**](https://ma-rconnect.lancs.ac.uk/one_sample_t){target="_blank"}